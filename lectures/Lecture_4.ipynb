{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias variance trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generative model:\n",
    "\n",
    "$y \\tilde{} N(Xw, \\sigma^2I)$\n",
    "\n",
    "Least squares:\n",
    "\n",
    "- $w_{LS} = (X^TX)^{-1}X^Ty$\n",
    "- $\\mathbb{E}[w_{LS}] = w$\n",
    "- $Var[w_{LS}] = \\sigma^2(X^TX)^{-1}$\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "- $w_{RR} = (\\lambda I + X^TX)^{-1}X^Ty$\n",
    "- $\\mathbb{E}[w_{RR}] = (\\lambda I + X^TX)^{-1}X^TXw$\n",
    "- $Var[w_{RR}] = \\sigma^2Z(X^TX)^{-1}Z^T$\n",
    "- $Z = (I + \\lambda(X^TX)^{−1})^{−1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating squared error\n",
    "\n",
    "$x_0$ and $y_0$ are new data\n",
    "\n",
    "$\\mathbb{E}[(y_0 − x^T_0\\hat{w})^2|X, x_0] = \\int_{R}\\int_{Rn}(y_0 − x^T_0\\hat{w})^2p(y|X, w)p(y_0|x_0, w)dydy_0$\n",
    "\n",
    "The distributions on y, y0 are Gaussian with the true (but unknown) w\n",
    "\n",
    "we're predicting the error if the real using our prediction, $\\hat{w}$\n",
    "\n",
    "$\\mathbb{E}[(y_0 − x^T_0\\hat{w})^2] = \\mathbb{E}[y^2_0] - 2\\mathbb{E}[y_0]x^T_0\\mathbb{E}[\\hat{w}] + x^T_0\\mathbb{E}[\\hat{w}\\hat{w}^T]x_0$\n",
    "\n",
    "Since y and w are independant, $\\mathbb{E}[y_0\\hat{w}] = \\mathbb{E}[y_0]\\mathbb{E}[\\hat{w}]$\n",
    "\n",
    "$\\mathbb{E}[\\hat{w}\\hat{w}^T] = Var[\\hat{w}] + \\mathbb{E}[\\hat{w}]\\mathbb{E}[\\hat{w}]^T$\n",
    "\n",
    "$\\mathbb{E}[y^2_0] = \\sigma^2 + (x_0^Tw)^2$\n",
    "\n",
    "plugging these in:\n",
    "\n",
    "$\\mathbb{E}[(y_0 − x^T_0\\hat{w})^2] = \\sigma^2 + x^T_0(w − \\mathbb{E}[\\hat{w}])(w − \\mathbb{E}[\\hat{w}])^Tx_0 + x^T_0Var[\\hat{w}]x_0$\n",
    "\n",
    "- $\\sigma^2$ is the noise, cant be controlled\n",
    "- $x^T_0(w − \\mathbb{E}[\\hat{w}])(w − \\mathbb{E}[\\hat{w}])^Tx_0$ is the squared bias, how close we expect to be on average\n",
    "- $x^T_0Var[\\hat{w}]x_0$ is the variance, how sensitive to teh data it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probability of A = $P(A)$\n",
    "\n",
    "probability of B = $P(B)$\n",
    "\n",
    "#### conditional\n",
    "\n",
    "probability of A given B is True = $P(A|B)$\n",
    "\n",
    "probability of B given A is True = $P(B|A)$\n",
    "\n",
    "#### joint\n",
    "\n",
    "probability of A and B = $P(A,B)$\n",
    "\n",
    "### Calculus\n",
    "\n",
    "$P(A, B) = P(A|B)P(B) = P(B|A)P(A)$\n",
    "\n",
    "$P(A) = \\sum_bP(A,B=b)$\n",
    "\n",
    "$P(B) = \\sum_aP(A=a,B)$\n",
    "\n",
    "#### We can use ths to find:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "$= \\frac{P(B|A)P(A)}{\\sum_aP(A=a,B)}$\n",
    "\n",
    "$= \\frac{P(B|A)P(A)}{\\sum_aP(B|A=a)P(A=a)}$\n",
    "\n",
    "and the swapping a and b:\n",
    "\n",
    "$P(B|A)= \\frac{P(A|B)P(B)}{\\sum_bP(A|B=b)P(B=b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes rule says that the probability of B after knowing A is\n",
    "\n",
    "$P(B|A)= \\frac{P(A|B)P(B)}{P(A)}$\n",
    "\n",
    "$P(B|A)$\n",
    "posterior\n",
    "\n",
    "$P(A|B)$\n",
    "likelihood\n",
    "\n",
    "$P(B)$\n",
    "prior\n",
    "\n",
    "$P(A)$\n",
    "marginal\n",
    "\n",
    "#### with densities (as it works with continous random values)\n",
    "\n",
    "Let θ be a continuous-valued model parameter\n",
    "\n",
    "Let X be data we possess\n",
    "\n",
    "$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{\\int p(X|\\theta)p(\\theta)d\\theta}=\\frac{p(X|\\theta)p(\\theta)}{p(X)}$\n",
    "\n",
    "$p(X|\\theta)$ is the likelihood, known from the model definition\n",
    "\n",
    "$p(\\theta)$ is a prior distribution that we define\n",
    "\n",
    "Given these two, we can (in principle) calculate $p(\\theta|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The likelihood model is $y \\tilde{} N(Xw, \\sigma^2I)$\n",
    "\n",
    "prior is a 0 mean gaussian distribution given to w:\n",
    "\n",
    "$w \\tilde{} N(0, \\lambda^{−1}I)$\n",
    "\n",
    "so the density function for w is:\n",
    "\n",
    "$p(w) = \\frac{λ}{2\\pi}^{\\frac{d}{2}}e^{− \\frac{\\lambda}{2}w^Tw}$\n",
    "\n",
    "we now find a w which satifies the likelihood and prior\n",
    "\n",
    "$w_{MAP} = \\arg \\max_w \\ln p(w|y, X)$\n",
    "\n",
    "$w_{MAP} = \\arg \\max_w \\ln \\frac{p(y|w, X)p(w)}{p(y|X)}$\n",
    "\n",
    "$w_{MAP} = \\arg \\max_w \\ln p(y|w, X) + \\ln p(w) - \\ln p(y|X)$\n",
    "\n",
    "$w_{MAP} = \\arg \\max_w \\ln p(y|w, X) + \\ln p(w)$\n",
    "\n",
    "$ = \\arg \\max_w −\\frac{1}{2\\sigma^2}(y − Xw)^T(y − Xw) −\\frac{\\lambda}{2}w^Tw + const$\n",
    "\n",
    "finding teh loss and minimizing it:\n",
    "\n",
    "$\\triangledown_wL = \\frac{1}{σ^2}X^Ty − \\frac{1}{σ^2}X^TXw − \\lambda w = 0$\n",
    "\n",
    "$w_{MAP} = (\\lambda\\sigma^2I + X^TX)^{-1}X^Ty$\n",
    "\n",
    "MAP = RR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
