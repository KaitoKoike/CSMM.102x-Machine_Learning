{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view of least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a diagonal covariance matrix $\\Sigma = \\sigma^2I$. The density is:\n",
    "\n",
    "$$p(y|\\mu,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^T(y-\\mu)\\right)$$\n",
    "\n",
    "if we set $\\mu = Xw$ and find maximum likelihood for w:\n",
    "\n",
    "$$w_{ML} = \\arg  \\underset{w}{\\max} \\ln p(y|\\mu = Xw,\\sigma^2)$$\n",
    "\n",
    "$$w_{ML} = \\arg  \\underset{w}{\\max} -\\frac{1}{2\\sigma^2}||y-Xw||^2 - \\frac{n}{2}\\ln(2\\pi\\sigma^2)$$\n",
    "\n",
    "we remove the last bit because it doesn't involve w\n",
    "\n",
    "$$w_{ML} = \\arg  \\underset{w}{\\max} -\\frac{1}{2\\sigma^2}||y-Xw||^2$$\n",
    "\n",
    "Using least squares:\n",
    "\n",
    "$$w_{ML} = \\arg  \\underset{w}{\\min}||y − Xw||^2$$\n",
    "\n",
    "These are both the same, so:\n",
    "\n",
    "$$w_{ML} = \\arg  \\underset{w}{\\max} -\\frac{1}{2\\sigma^2}||y-Xw||^2 \\Leftrightarrow \\arg  \\underset{w}{\\min}||y − Xw||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are making an independent Gaussian noise assumption about error:\n",
    "\n",
    "$\\epsilon_i = y_i - x^T_iw$\n",
    "\n",
    "or other ways to say same thing:\n",
    "\n",
    "$y_i = x^T_iw + \\epsilon_i$, $\\epsilon_i\\overset{iid}{\\tilde{}} N(0, \\sigma^2)$, for $i = 1,...,n$\n",
    "\n",
    "$y_i\\overset{ind}{\\tilde{}} N(x^T_iw, \\sigma^2)$, for $i = 1,...,n$\n",
    "\n",
    "$y \\tilde{} N(Xw, \\sigma^2I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our model assumption is $y \\tilde{} N(Xw, \\sigma^2I)$\n",
    "\n",
    "$\\mathbb{E}[w_{ML}] = \\mathbb{E}[(X^TX)^{-1} X^Ty]$\n",
    "\n",
    "or $\\mathbb{E}[w_{ML}] = \\int_{}[(X^TX)^{-1} X^Ty] p(y|X, w)dy = \\mu$\n",
    "\n",
    "$\\mathbb{E}[w_{ML}] = (X^TX)^{-1} X^T\\mathbb{E}[y]$\n",
    "\n",
    "$\\mathbb{E}[w_{ML}] = (X^TX)^{-1} X^TXw$\n",
    "\n",
    "$\\mathbb{E}[w_{ML}] = w$\n",
    "\n",
    "$w_{ML}$ is an un-biased estimate of w\n",
    "\n",
    "the expected solution = the correct one\n",
    "\n",
    "but if the variance is too high, we cant expect a value near the solution\n",
    "\n",
    "### covariance\n",
    "\n",
    "$Var[y] = \\mathbb{E}[(y - \\mathbb{E}[y])(y - \\mathbb{E}[y])^T] = \\Sigma$\n",
    "\n",
    "plugging in $\\mathbb{E}[y] = \\mu$\n",
    "\n",
    "$Var[y] = \\mathbb{E}[(y - \\mu)(y - \\mu)^T]$\n",
    "\n",
    "$= \\mathbb{E}[yy^T - y\\mu^T - \\mu y^T + \\mu\\mu^T]$\n",
    "\n",
    "$= \\mathbb{E}[yy^T] - \\mu\\mu^T = \\Sigma$\n",
    "\n",
    "$= \\mathbb{E}[yy^T] = \\Sigma + \\mu\\mu^T$\n",
    "\n",
    "### Variance of solution\n",
    "\n",
    "with least squares regression, we need to find:\n",
    "\n",
    "$Var[w_{ML}] = \\mathbb{E}[(w_{ML} - \\mathbb{E}[w_{ML}])(w_{ML} - \\mathbb{E}[w_{ML}])^T]$\n",
    "\n",
    "$= \\mathbb{E}[w_{ML}w^T_{ML}] - \\mathbb{E}[w_{ML}]\\mathbb{E}[w_{ML}]^T$\n",
    "\n",
    "plugging in our previous equalities:\n",
    "\n",
    "$Var[w_{ML}] = \\mathbb{E}[(X^TX)^{-1} X^Tyy^TX (X^TX)^{-1}] - ww^T$\n",
    "\n",
    "$= (X^TX)^{-1} X^T\\mathbb{E}[yy^T]X (X^TX)^{-1} - ww^T$\n",
    "\n",
    "$= (X^TX)^{-1} X^T(\\sigma^2I + Xww^TX^T)X (X^TX)^{-1} - ww^T$\n",
    "\n",
    "$= (X^TX)^{-1} X^T\\sigma^2IX (X^TX)^{-1} + (X^TX)^{-1} X^TXww^TX^TX (X^TX)^{-1} - ww^T$\n",
    "\n",
    "$= \\sigma^2(X^TX)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so with model $y \\tilde{} N(Xw, \\sigma^2I)$:\n",
    "\n",
    "$\\mathbb{E}[w_{ML}] = w$\n",
    "\n",
    "$Var[w_{ML}] = \\sigma^2(X^TX)^{-1}$\n",
    "\n",
    "this means that if there are very large values in the variance $\\sigma^2(X^TX)^{-1}$ the values of $w_{ML}$ are very sensitive to the measured data y\n",
    "\n",
    "this is bad if we want to use $w_{ML}$ to predict and analyze because we aren't confident in the maximum likelihood of w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "the values in $w_{ML}$ can be huge so we constrain the model parameters\n",
    "\n",
    "were using form:\n",
    "\n",
    "$w_{OPT} = arg \\underset{w}{\\min}||y - Xw||^2 + \\lambda g(w)$\n",
    "\n",
    "- $\\lambda > 0$: a regularization parameter\n",
    "- $g(w) < 0$: a penalty function which encourages a desired property/properties from w\n",
    "\n",
    "ridge regression uses the squared penalty $g(w) = ||w||^2$\n",
    "\n",
    "$w_{RR} = arg \\underset{w}{\\min}||y - Xw||^2 + \\lambda||w||^2$\n",
    "\n",
    "this penalizes large values of w\n",
    "\n",
    "the trade off between main term and penalty term is controlled by $\\lambda$\n",
    "\n",
    "- Case $\\lambda \\to 0 : w_{RR} \\to w_{LS}$\n",
    "- Case $\\lambda \\to \\infty : w_{RR} \\to \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solving ridge regression\n",
    "\n",
    "same procedure as least squares\n",
    "\n",
    "$L = ||y - Xw||^2 + \\lambda||w||^2$\n",
    "\n",
    "$L = (y - Xw)^T(y - Xw) + \\lambda w^Tw$\n",
    "\n",
    "set gradient of loss to 0\n",
    "\n",
    "$\\triangledown_w L = -2X^Ty + 2X^tXw + 2\\lambda w = 0$\n",
    "\n",
    "$w_{RR} = (\\lambda I + X^TX)^{-1}X^Ty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a trade off between the squared error and the penalty\n",
    "\n",
    "we write each term as level sets,  Curves where function evaluation gives the same number\n",
    "\n",
    "The sum of these gives a new set of levels with a unique minimum\n",
    "\n",
    "$||y−Xw||^2 +\\lambda ||w||^2 = (w−w_{LS})^T(X^TX)(w−w_{LS})+\\lambda w^Tw+ (const. w.r.t. w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Because the weights are limited,\n",
    "\n",
    "We assume this preprocessing has been applied before:\n",
    "\n",
    "we take away the mean:\n",
    "\n",
    "$y \\gets y - \\frac{1}{n}\\sum^n_{i=1}y_i$\n",
    "\n",
    "we standardize the dimensions of $x_i$:\n",
    "\n",
    "$x_{ij} \\gets (x_{ij} - \\bar{x}_{\\cdot j})/\\hat{\\sigma}_j$\n",
    "\n",
    "$\\hat{\\sigma}_j = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}(x_{ij} - \\bar{x}_{\\cdot j})^2}$\n",
    "\n",
    "this subtracts the empirical mean and divides by the empirical standard\n",
    "deviation for each dimension\n",
    "\n",
    "this means there is no need for the dimension of 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysis of ridge regression\n",
    "\n",
    "$w_{RR} = (\\lambda(X^TX)^{-1} + I)w_{LS}$\n",
    "\n",
    "$||w_{RR}||_2 \\le ||w_{LS}||_2$\n",
    "\n",
    "$w_{RR}= V(\\lambda S^{−2} + I)^{−1}V^Tw_{LS}$\n",
    "\n",
    "$w_{RR}= VMV^Tw_{LS}$\n",
    "\n",
    "$M_{ii} = \\frac{s^2_{ii}}{\\lambda + s^2_{ii}}$\n",
    "\n",
    "can add $\\sqrt{\\lambda}$ diagonal matrix to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bff3c581c5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.arange(5).reshape(-1)\n",
    "\n",
    "print np.square(a)\n",
    "\n",
    "print np.matmul(np.transpose(a), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
