{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = \\ln\\frac{p(y = +1|x)}{p(y = -1|x)}$\n",
    "\n",
    "- L >> 0: more confident in y = +1\n",
    "- L << 0: more confident in y = -1\n",
    "- L = 0: dunno\n",
    "\n",
    "Linear function $x^Tw + w_0$ captures:\n",
    "- $\\left|\\frac{x^Tw}{||w||_2} + \\frac{w_0}{||w||_2}\\right|$ gives us distance from hyperplane to x\n",
    "- sign of function captures which side x is on\n",
    "- As x moves away/towards H, we become more/less confident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the previous bayes classifier had a prior on y, and distribution on data x so the weights were restricted\n",
    "\n",
    "$p(y = -1|x) = 1 - p(y = +1|x)$\n",
    "\n",
    "$p(y = +1|x) = \\frac{\\exp\\{x^Tw + w_0\\}}{1 + \\exp\\{x^Tw + w_0\\}} = \\sigma(x^Tw + w_0)$\n",
    "- called sigmoid function\n",
    "\n",
    "with offset absorbed into x and w\n",
    "\n",
    "$P(y_i = +1|x_i, w) = \\sigma(x^T_iw)$\n",
    "\n",
    "$\\sigma(x^T_iw) = \\frac{e^{x^T_iw}}{1 + e^{x^T_iw}}$\n",
    "\n",
    "#### this is discrimitive because x is not directly modeled\n",
    "\n",
    "- discriminitive: p(y|x)\n",
    "- discriminitive: p(x|y)p(y)\n",
    "- bayes a generative because x is modeled\n",
    "\n",
    "#### Joint Likelihood\n",
    "\n",
    "$\\prod^n_{i=1}p(y_i|x_i, w)$\n",
    "\n",
    "$= \\prod^n_{i=1}\\sigma(x^T_iw)^{\\mathbb{1}(y_i = +1)}(1-\\sigma(x^T_iw))^{\\mathbb{1}(y_i = -1)}$\n",
    "- the joint probability (using the sigmoid function) of each value\n",
    "\n",
    "$= \\left(\\frac{e^{x^T_iw}}{1 + e^{x^T_iw}}\\right)^{\\mathbb{1}(y_i = +1)}\\left(1-\\frac{e^{x^T_iw}}{1 + e^{x^T_iw}}\\right)^{\\mathbb{1}(y_i = -1)}$\n",
    "- this is our confidence\n",
    "\n",
    "$= \\frac{e^{y_ix^T_iw}}{1 + e^{y_ix^T_iw}}$\n",
    "- this is $\\sigma(y_i\\cdot x^T_iw)$\n",
    "- we want to maximize this over w\n",
    "\n",
    "$w_{ML} = \\arg\\max_w\\sum^n_{i=1}\\ln\\sigma(y_i\\cdot x^T_iw)$\n",
    "\n",
    "$w_{ML} = \\arg\\max_wL$\n",
    "\n",
    "we cant set $\\triangledown_wL = 0$\n",
    "\n",
    "$w\\prime = w + \\eta\\triangledown_wL$\n",
    "\n",
    "$\\triangledown_wL = \\sum^n_{i=1}(1 - \\sigma(y_i\\cdot x^T_iw))y_ix_i$\n",
    "- its like the peceptron\n",
    "- but we're multiplying the addition by our unconfidence\n",
    "\n",
    "$w^{(t+1)} = w^{(t)} + \\eta\\sum^n_{i=1}(1 - \\sigma(y_i\\cdot x^T_iw))y_ix_i$\n",
    "\n",
    "#### for linearly seperable data:\n",
    "\n",
    "because $||w||_2 \\to \\infty$ so that $L \\to 1$\n",
    "\n",
    "#### for nearly linear seperable data:\n",
    "\n",
    "overfitting where $||w||_2 \\to \\infty$ so that $L \\to 1$ for majority of values but some wont be accurate\n",
    "\n",
    "### Regularization\n",
    "\n",
    "$w_{map} = \\arg\\max_w\\sum^n_{i=1}\\ln\\sigma(y_i\\cdot x^T_iw) - \\frac{\\lambda}{2}w^T w$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
